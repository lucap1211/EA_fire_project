{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earth Analytics Final Project\n",
    "## 2024 Summer\n",
    "\n",
    "## Project Overview\n",
    "For my final project I am interested in exploring the narrative text fields of the ICS-209-PLUS-WILDFIRE dataset. The ICS-209-PLUS-WILDFIRES is a fire-focused subset of the [all-hazards dataset](https://www.nature.com/articles/s41597-023-01955-0) mined from the US National Incident Management System 1999–2020 by St. Denis et al. (2023).  I will process the narrative fields to convert text about societal impacts into a suitable format for natural language processing and analyze text to find links between fire hazard characteristics, incident response, and societal impacts/threats incrementally across all phases of active response. My main question that drives the anaysis is:\n",
    "\n",
    "How can we connect societal impacts and geophysical metrics by topic modeling methods on incidence reporting's narrative fields?\n",
    "\n",
    "<center>\n",
    "    <img src=\"graphics/workflowwhite.png\" alt=\"Project Workflow\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "## Installation\n",
    "The project will run in the public [earth-analytics-python-env](https://github.com/earthlab/earth-analytics-python-env) that contains the dependencies and libraries needed for the project. If other libraries becomes neccessary for the project, they will be listed here as additional requirements. \n",
    "\n",
    "## Data\n",
    "For my project, I plan to heavily focus on text-based narrative data (ICS-209-PLUS-WILDFIRES) and use geospatial data (like Monitoring Trends in Burn Severity).\n",
    "\n",
    "### ICS-209-PLUS-WILDFIRES\n",
    "\n",
    "- Standardized, on-scene Incident Status Summary\n",
    "- Part of the National Incident Management System (NIMS)\n",
    "- Text-based narrative wealth of data\n",
    "- Science-grade situation reports focusing on large wildfires\n",
    "- Daily “informational snapshots” of fire response/management\n",
    "- View into the decision-making process\n",
    "    - Large fire event development and response\n",
    "\n",
    "### Monitoring Trends in Burn Severity\n",
    "\n",
    "The Monitoring Trends in Burn Severity (MTBS) dataset is a comprehensive dataset that maps the fire severity and perimeters of large wildfires in the United States across all ownerships. The MTBS vector datasets include burn scar boundaries that are delineated from satellite imagery and burn severity index data at a map scale of 1:24,000 to 1:50,000. I will primarily use the vector burn area boundaries.\n",
    "\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Text preprocessing is a crucial step in Natural Language Processing (NLP) that transforms text into a format that is suitable for further analysis. Here are some common techniques:\n",
    "\n",
    "1. **Tokenization**: This is the process of breaking down text into individual words (or tokens). This is usually the first step in text preprocessing.\n",
    "\n",
    "    ```python\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    ```\n",
    "\n",
    "2. **Lowercasing**: This is done to avoid having multiple copies of the same words. For example, 'Hello' and 'hello' should be treated as the same word.\n",
    "\n",
    "    ```python\n",
    "    text = text.lower()\n",
    "    ```\n",
    "\n",
    "3. **Stopwords Removal**: Stopwords are common words that do not contain important meaning and are usually removed from texts. Examples of stopwords are 'is', 'the', 'and', etc.\n",
    "\n",
    "    ```python\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in tokens if word not in stop_words]\n",
    "    ```\n",
    "\n",
    "4. **Stemming**: This is the process of reducing inflected (or sometimes derived) words to their word stem or root form. For example, 'jumps', 'jumping', 'jumped' are all transformed to 'jump'.\n",
    "\n",
    "    ```python\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "    ```\n",
    "\n",
    "5. **Lemmatization**: Similar to stemming, but it reduces words to their base or root form (lemma) considering the context. It's more accurate but slower than stemming.\n",
    "\n",
    "    ```python\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    ```\n",
    "\n",
    "6. **Removing Punctuation**: Punctuation can provide grammatical context to a sentence which supports our understanding. But for our vectorizer which counts the number of words and not the context, it does not add value, so it is often removed.\n",
    "\n",
    "    ```python\n",
    "    import string\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    ```\n",
    "\n",
    "7. **Removing HTML tags**: When dealing with HTML data, we often have to clean it to remove all the HTML tags in it.\n",
    "\n",
    "    ```python\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
